\chapter{Applicativo}

\section{Struttura}

L'interfaccia è stata realizzata utilizzando il linguaggio
di programmazione Python e la libreria PyQt versione 5.
PyQt è un'implementazione Python per Qt, un framework
multi-piattaforma per lo sviluppo di applicazioni grafiche.
Le sue principali caratteristiche sono la portabilità,
l'interoperabilità e la facilità di utilizzo.
Questo permette di poter impiegare lo stesso codice
su più sistemi operativi senza dover aggiungere
ulteriori livelli di complessità o l'utilizzo
di strumenti di virtualizzazioni.

Ai fini di testare l'applicativo sarebbe necessario
avere a disposizione un dispositivo ISCAN da poter collegare
al computer e vedere il comportamento dell'applicativo con
la sorgente video catturata.
Questo è ovviamente impratico per vari motivi,
dal dover impegnare lo strumento per lunghi periodi di tempo
alla necessità di avere un paziente che si stia sottoponendo
ad un esame in modo da avere un responso.
Per evitare queste problematiche, per fini puramente
dimostrativi, è stato realizzato anche uno script
per simulare la sorgente video dell'ISCAN, utilizzando
il software {\tt ffmpeg} e l'ecosistema {\tt V4L}.
{\tt V4L} è l'abbreviazione di {\it Video for Linux},
e permette la creazione di dispositivi video virtuali
su cui trasmettere un flusso video arbitrario, che verrà
percepito dal sistema operativo come un dispositivo video
reale, ad esempio una webcam o lo stesso ISCAN.

L'applicativo deve quindi principalmente essere in grado
di catturare un flusso video, in modo da poterne estrarre
i fotogrammi e ottenerne le predizioni dal modello.
Dalla predizione ottenuta bisognerà poi, tenendo conto
delle impostazioni scelte, ricreare l'immagine finale
da mostrare all'operatore.
Il modello in questo caso non viene istanziato direttamente
all'interno dell'applicazione, ma viene esposto come servizio
esterno con cui l'applicativo comunica tramite una API REST.
Le API REST sono un modo di comunicare con un servizio web
tramite richieste HTTP, la cui principale caratteristica
è la semplicità di utilizzo, e per questo molto popolare.
Questo tuttavia non significa che le immagini provenienti
dalla sorgente video escano dal computer e vengano
inviate in rete, in quanto il {\it server} che gestisce
tali richieste viene istanziato localmente e quindi
la comunicazione è limitata al computer su cui si sta
utilizzando l'applicativo.
Tuttavia sarebbe possibile estendere tale funzionalità in modo
da avere un servizio, esposto possibilmente su una rete locale,
che si occupa di gestire le richieste provenienti da più
applicativi, operazione che potrebbe essere interessante
per ridurre ulteriormente i costi o raccogliere ulteriori
dati per seguenti migliorie.

Il server che gestisce l'istanza del modello puo' essere
attivato tramite la libreria {\tt MMDeploy}\cite{mmdeploy},
facente parte dell'ecosistema {\tt OpenMMLab},
e che si occupa di semplificare la fase di {\it serving}
di modelli sviluppati con {\tt MMSeg} o altre librerie
facenti parte del progetto.
Come accennato, la libreria {\tt MMSeg} è costruita
al di sopra della libreria {\tt PyTorch}, che a sua volta
offre un suo software per il {\it serving} dei modelli
denominato {\tt torchserve}.
Tuttavia il formato impiegato da {\tt MMSeg} per esportare
i modelli, ovvero {\tt ONNX}, è diverso da quello di {\tt PyTorch},
e tale modello necessita quindi di essere convertito.
Una volta convertito il modello, {\tt MMDeploy} mette a
disposizione un'immagine {\tt Docker} per creare un'istanza
del modello sulla macchina, che a sua volta utilizzerà la
libreria {\tt torchserve}.
Uno script {\it ad-hoc} è stato creato per semplificare questa
fase, ma va fatto notare che una volta ottenuto un modello
soddisfacente è possibile derivare un'immagine derivata
da quella impiegata che contenga al suo interno il modello
in modo da semplificare drammaticamente questa fase.
I principali vantaggi dell'impiego di questa architettura
sono nuovamente la portabilità e la semplicità d'uso.
Avviata l'istanza sarà possibile inviare richieste HTTP
all'indirizzo preimpostato allegando l'immagine da analizzare,
da cui otterremo come rispsosta un'altra immagine contenente
la maschera predetta dal modello.

L'applicativo internamente utilizza un timer per catturare
i fotogrammi in modo da poterli inviare al modello.
Allo scadere del timer l'applicativo cattura un fotogramma
dalla sorgente video scelta e ne richiede la predizione che,
dopo essere stata visualizzata, farà ripartire il timer.
La frequenza con cui ciò accade è stata impostata 40
millisecondi, che corrisponde a circa 24 fotogrammi al secondo,
una frequenza video standard considerata sufficientemente fluida.
Tale scelta è dovuta a limitiazioni hardware, nel caso in cui
una precedente richiesta non sia stata ancora soddisfatta
non ne verranno inviate di nuove, ma è anche possibile, nel
momento in cui si disponga di hardware sufficientemente potente,
impostare il timer a valori più bassi o direttamente richiedere
che una nuova richiesta venga inviata non appena la precedente
è stata soddisfatta.


\section{Interfaccia}

L'interfaccia è stata sviluppata tenendo a mente gli obiettivi
funzionali preposti, ovvero la semplicità d'uso e la velocità
nel processo di avvio.
All'avvio dell'applicativo una prima interfaccia chiede di
scegliere la sorgente video da utilizzare.
L'accesso a una sorgente video avviene tramite richiesta
al suo ID, che è un numero intero che lo identifica.
Tali ID sono progressivi a partire da 0, ovvero il primo
dispositivo ha ID uguale a 0.

\begin{figure}[H]
    \includegraphics[width=0.9\textwidth]{./assets/app-splashscreen.png}
    \caption{\label{fig:app-splashscreen} Finestra selezione ingresso video}
\end{figure}

Questo passaggio è necessario poiché non è noto a priori
quale sia l'ID dello strumento utilizzato dal personale medico,
che dipende dall'ordine in cui i sistemi operativi assegnano
questo ID all'accensione e al momento in cui il dispositivo
viene collegato al computer.
Per scegliere una sorgente video è sufficiente premere
il bottone {\tt Usa} accanto al suo ID, e per facilitare
tale operazione una piccola anteprima in tempo reale
viene mostrata in corrispondenza di ogni sorgente video
disponibile, come visibile in figura \ref{fig:app-splashscreen}.

Questo step esclude dal processo di selezione le sorgenti video
non valide o non disponibili.
Sempre in figura \ref{fig:app-splashscreen} infatti notiamo come
siano disponibili le sorgenti video 1 e 3, che corrispondono
ai dispositivi video 0 e 2, siano riportati in questa
finestra di selezione, mentre il dispositivo 1 e di conseguenza
la sorgente video 2 non è selezionabile.
Questo accade perché nella postazione di sviluppo dell'interfaccia
il dispositivo 1 è in realtà un'interfaccia virtuale del
dispositivo 0 e come tale non puo' essere impiegata per
l'acquisizione di immagini.

Selezionando una delle opzioni la finestra viene quindi chiusa
e si apre una seconda finestra che mostra il video selezionato
e dà l'accesso alle funzionalità dell'applicativo.
Da questo momento l'applicativo è gia in funzione
e al di sopra dell'uscita video vengono mostrare le predizioni
del modello.
Gli unici ulteriori controlli che l'utente puo' modificare a
suo piacimento sono degli {\it slider} che permettono di
regolare due parametri delle predizioni, che sono
l'opacità e lo spessore della maschera creata.
Questi due parametri sono stati resi modificabili in modo
da poter scegliere una combinazione che risulti
in maschere di predizioni visibili in modo chiaro e allo
stesso tempo che non coprano porzioni troppo elevate di
immagine o siano troppo distraenti.


