\graphicspath{../assets}

\chapter{Modello}
\section{Lavori Precedenti e Considerazioni Generali}

La terminologia intelligenza artificiale, o alternativamente machine learngin,
indica un campo molto vasto che raccoglie molte e diverse tecniche di apprendimento
ed elabroazione dati, che possono spaziare su più livelli di complessità ed essere 
divisi in molteplici categorie.
Un primo distinguo che si puo' fare nel merito delle tecniche di apprendimento è quello
tra {\it shallow learning} e {\it deep learning}.
Il primo si potrebbe tradurre in italiano come "apprendimento superficiale",
mentre il secondo come "apprendimento profondo". \\
Seppure le differenze tra i due siano tante e non di poco conto, in entrambi i casi
abbiamo un algoritmo che dato un input $\vec{x}$, la cui realizzazione concreta
varia a seconda dell'approccio utilizzato e specifichieremo nel dettaglio quando
necessario, e ne calcolano un output $\vec{y} = f(\vec{x})$.
Anche per $\vec{y}$ la realizzazione concreta puo' differire molto, 
ma una forma generale è un vettore con $N_c$ dimensioni, $N_c$, dove $N_c$ è il numero di
classi del problema, ovvero il numero di possibili risposte.
Nel nostro specifico caso, dove cerchiamo soltanto di discriminare tra un tessuto
ritenuto sano ed uno ritenuto patologico, avremmo che $N_c = 2$.
Nel caso si stia trattando un problema binario come il nostro, un'altra alternativa
è quella di utilizzare un output di dimensione $N_c = 1$, che può essere un
vettore unidimensionale o uno scalare.


\begin{figure}
    \center
    \includegraphics[width=0.9\textwidth]{./assets/semseg.jpg}
    \caption{\label{fig:semseg}Esempio di Semantic Segmentation applicata al riconoscimento di oggetti stradali. Fonte: https://towardsai.net/p/l/machine-learning-7}
\end{figure}

Il problema che si tenta di risolvere è denominato
{\it semantic segmentation}, di cui un esempio è visibile nella figura 
\ref{fig:semseg} un caso specifico della {\it image classification}.
Nella {\it image classification} infatti quel che si fa è dedurre cosa l'immagine
in input rappresenti, senza specificare quale parte dell'immagine contenga la classe
rilevata.
Nella {\it semantic segmentation} invece si vuole rilevare quale zona dell'immagine
contenga le classi rilevate. 
L'utilizzo del plurale nell'ultima frase non è casuale: se infatti nella {\it image classification}
un'immagine contnente più oggetti risponde generalmente con una sola classe,
nella {\it semantic segmentation} si gestisce anche il caso in cui siano presenti più
oggetti di diverse classi, come nel nostro caso in cui un'immagine contente un
tessuto potenzialmetne patologico conterrà anche parti di tessuto sano o
porzioni d'immagine che non rappresentano affatto dei tessuti.
Per questo motivo il nostro problema binario avrà come classi la classe $C_1$, che indicherà
le zone di tessuto ritenute come potenzialmente patologiche ({\it "sick"}) e la classe $C_0$,
che denomineremo {\it "sane"} in maniera impropria dato che comprenderà
anche la parte di sfondo dell'immagine.
Questo è dovuto al fatto che i dataset impiegati, spiegati nel dettaglio in 
\ref{sec:Dataset}, hanno la sola annotazione delle zone malate, e non distinguono tra
zone sane e sfondo.


\subsection{\label{sec:shallow-learning}Tecniche Shallow Learning: Alberi Decisionali}

Il nostro primo modello di riferimento\cite{Toma2022} è basato su
alberi decisionali, modelli molto popolari nel mondo del machine learning
per vari motivi, uno tra i quali è la capacità di poter interpretare il loro funzionamento.
Infatti se con modelli {\it deep learning} non è possibile, se non in
modo ristretto e con poche certezze, capire il processo di inferenza, 
gli alberi decisionali per costruzione sono molto
vicini al pensiero umano, e quindi facilmente interpretabili.
Modelli di questo tipo prendono anche il nome di modelli {\it white box},
mentre modelli non interpretabili prendono il nome di modelli {\it black box}.
Tuttavia, questa affermazione ha una validità relativa, in quanto
nel momento in cui i valori del campione in ingresso, vale a dire le
componenti del nostro vettore $\vec{x}$ salgono in numero e complessità,
interpretare il procedimento di inferenza diventa sempre più difficile non per
come il modello prende la decisione ma perché è difficile per le persone
valutare la decisione sulla base di valori statistici.

Il lavoro in questione non ha però semplicemente utilizzato degli decisionali,
ma piuttosto un {\it ensamble} di alberi decisionali, che prende il nome
di {\it random forest}, ovvero un insieme di un grande numero di alberi decisionali.
Questa è una metodologia molto diffusa sopratutto quando, per la difficoltà del
problema o per la scarsità dei dati, non si riesce a costruire un singolo
albero dai risultati soddisfacenti.
Quel che accade è che allora si utilizzano un numero elevati di alberi,
nel concetto generico qui descritti come {\it weak learners}, apprenditori
deboli, e ogni modello esprime la propria valtuazione sul campione.
Viene poi messo in atto un procedimento per ottenere un unico risultato finale,
che nel caso del lavoro descritto è il {\it majority vote}, dove si sceglie
il risultato che la maggioranza dei singoli alberi ha predetto.

\subsection{Algoritmo I}

Al fine di valutare i pro e i contro di questa metodologia, l'algoritmo
è stato reimplementato con alcune modifiche e lievi ottimizzazioni, assieme ad una
GUI per valutarne la praticità d'uso.

Come nel lavoro originale, l'immagine viene convertita in scala di grigi,
e si saltà il ritaglio delle immadini dal momento che esse sono già state
ritagliate in precedenza.
Dopodiché si applia comunque l'algoritmo di {\it K-Means} per rimuovere
i riflessi di luce.
In questo caso per velocizzare il processo si utilizza comunque un numero
$K=30$, con la differenza che non si utilizza un'inizializzazione
casuale ma si parte da una suffivisiione equispaziata da 0 a 255, ovvero
il massimo e il minimo valore di intensità dei pixel, oltre a iterare
il procedimento una sola volta.
Si rimuovono i pixel che ricadono nei tre cluster a intensità maggiore.
Infine si applica il metodo del gradiente di Sobel per evidenziare i bordi
e le discontinuità nelle venature del tessuto.
Dall'immagine ottenuta si estraggono delle {\it ROI} ({\it Region of Interest}),
anche dette {\it patches}, che sono delle porzioni dell'immagine di dimensione
fissa 50$\times$50 pixel, con una sovrapposizione del 40\% (20 pixel), ovvero
alcuni pezzi di una ROI fanno anche parte di altre ROI adiacenti.
Un esempio di questo tipo di sovrapposizione è mostrato in Figura \ref{fig:patch-overlap}.

\begin{figure}
    \includegraphics[width=\textwidth]{./assets/path-overlap.png}
    \caption{\label{fig:patch-overlap}Esempio di sovrapposizione delle ROI}
\end{figure}

Da qui vengono estratti i primi 11 valori statistici che andranno a comporre
i primi 11 valori del nostro vettore $\vec{x}$, che rappresenta la ROI in
ingresso.
A questi vengono poi agginti valori estratti da due matrici che vengono
calcolate dalla ROI stessa, la {\it Gray-Level Co-occurrence Matrix} (GLCM)
e la {\it Gray-Level Run Length Matrix} (GLRLM), che hanno prodotto
risultati soddisfacenti quando applicate ad altri problemi di
{\it pattern recognition}.\cite{GLCM}\\
In entrambi i casi bisogna anzitutto portare l'immagine da scala di grigi
a livelli di grigi.
Un'immagine bianco e nero è composta da un solo canale, con valori che possono
essere rappresentati come numeri interi tra 0 e 255.
Nel caso più generico potremmo dire che un'immagine in bianco e nero
è un'immagine che ha 256 livelli di grigi, ma questo non è ottimale al
funzionamento delle matrici, per cui si digitalizza l'immagine a un numero molto
minore di livelli di grigio, in questo caso 4.
Questo significa che, ai fini del calcolo delle matrici, i valori di grigio
dell'immagine tra 0 e 63, varrà 1 tra 64 e 127, e così via.\\
La GLCM è una matrice quadrata di dimensione $N \times N$, dove $N$ è il
numero di livelli di grigio, che contiene i valori di frequenza con cui
si ripetono coppie di pixel vicini.
È caratterizzata da due parametri, la {\it distanza} $d$ e l'{\it angolo} $\theta$,
ed è possibile utilizzarne molteplici per calcolare più matrici,
ma in questo caso è stata utilizzata una sola matrice, con $d=1$ e $\theta=0$°.
La distanza indica a che distanza cercare la co-occorrenza, nel caso di $d=1$
significa guardare l'elemento adiacente, mentre l'angolo indica la direzione
di adiacenza, in questo caso $0$° quello a destra dell'elemento considerato.

\begin{figure}[h]
    \center
    \includegraphics[width=0.9\textwidth]{./assets/glcm.jpg}
    \caption{Illustrazione del Calcolo di una GLCM a 8 livelli di grigio, fonte: https://it.mathworks.com/help/images/create-a-gray-level-co-occurrence-matrix.html}
\end{figure}

Da tale matrice si vanno ad estrarre altri 4 valori statistici.
Segue poi un processo simile per la GLRLM, anch'essa di dimensione
$N \times N$, dove $N$ è nuovamente il numero di livelli di grigio,
ma che è caratterizzata da un solo parametro, l'angolo $\theta$,
che indicia la direzione verso cui viene misurata una {\it run}.
In tale matrice, l'elemento $g_{i,j}$ rappresenta il numero di {\it run}
di lunghezza $j$ percorse nella direzione $\theta$ composte tutte dal medesimo
livello di grigio $i$.
Una rappresentazione visiva del calcolo è riportata nella Figura \ref{fig:glrlm}.


Dalla GLRLM si estraggono ulteriori 11 valori statistici.
Infine, l'ultima trasformazione da applicare all'immagine è la traduzione in
dominio di frequenza tramite la {\it Discrete Fourier Transform} (DFT), da cui
si estraggono gli ultimi 3 valori statistici portando il totale a 32.
In tabella \ref{table:features-toma} il dettaglio dei valori statistici impiegati.

\begin{table}
        \center
        \begin{tabular}[h]{||c|c|c|c||}
            \hline 
            \textbf{Primo Ordine} & \textbf{GLCM} & \textbf{GLRLM} & \textbf{DFT} \\
            \hline 
            \hline 
            Media & Contrasto & GLN & Freq. Massima \\
            Mediana & Correlazione & GLNN & Banda \\
            10 Percentile & Energia & HGLRE & Freq. di Kurtosis \\
            90 Percentile & Omogeneità & LGLRE & \\
            Diff. Interquantile & & LRE & \\
            Media Quadratica & & LRHGLE & \\
            Dev. Standard & & LRLGE & \\
            Varianza & & MAD & \\
            Uniformità & & RLN & \\
            {\it Skewness} & & RLNN & \\
            & & RP & \\
            & & SRE & \\
            & & SRHGLE & \\
            & & SRLGLE & \\
            \hline 
        \end{tabular}
        \caption{\label{table:features-toma}Valori statistici estratti dalle ROI.}
    % \end{center}
\end{table}

I valori raccolti da ogni singola ROI vengono raccolti e normalizzati tramite
{\it min-max}. 
Indicando con $max(x,i)$ il valore massimo dell'$i$-esima componente
del vettore $\vec{x}$ presente nel nostro dataset, ed analogamente
per $min(x,i)$, si normalizzano i vettori ricavati da equazione \ref{eq:minmaxnorm}.

\begin{equation}\label{eq:minmaxnorm}
    \vec{x}_{norm} = \frac{\vec{x} - min(x,i)}{max(x,i) - min(x,i)}
\end{equation}

Questo viene fatto perché le varie componenti possono variare molto tra loro
in termini di magnitudine, e potrebbero quindi ingannare gli alberi
a credere che variando di più abbiano un significato maggiore e quindi una
maggiore rilevanza nella classificazione.\\
A differenza di quanto svolto inizialmente in \cite{Toma2022}, la divisione
dei campioni non viene effettuata a posteriori ma a priori:
inizialmente si raccoglievano tutti i campioni e si divdeva in tre parti,
{\it train}, {\it validation} e {\it test}, ma in questo caso si è deciso di optare
per una divisione iniziale delle immagini in questi tre set in modo che
il modello non possa avere indizi extra su una ROI che deve valutare
in fase di test basandosi su una somiglianza di una ROI adiacente o 
parzialmente sovrapposta cui ha avuto accesso in fase di train.

\begin{figure}[h]
    \center
    \includegraphics[width=0.9\textwidth]{./assets/glrlm.jpg}
    \caption{\label{fig:glrlm}Calcolo di una GLRLM a 3 livelli di grigio con angolo $\theta$ uguale a 0°, fonte: http://www.ajnr.org/content/36/7/1343/F2}
\end{figure}

A questo punto ogni ROI viene classificata e a partire da essa viene
costruita una maschera globale dell'immagine.
Visto che le ROI vengono estratte con una sovrapposizione spaziale del 40\%,
potremmo avere che alcuni pixel vengano classificati contemporaneeamente
come patologiche che come sane.
Per risolvere questo problema si è classificato il singolo pixel tramite
coefficiente di Dice con soglia impostata a 0.5, che si traduce
praticamente in una classificazione come appartente a una data classe
se la maggioranza delle classificazioni appartengono a quella data classe.
Vengono infine applicati degli operatori morfologi di chiusura, erosione e dilazione
per ottenere una maschera meglio formata.

\begin{figure}[h]
    \center
    \includegraphics[width=0.9\textwidth]{./assets/gui-one.png}
    \caption{\label{fig:gui-one}Interfaccia grafica per la classificazione delle ROI. La zona cerchiata in verde indica l'area che desidereremmo rilevare, le zone a sovrapposizione rossa indicano le zone rilevate come anomale.}
\end{figure}

Presentata questa possibile implementazione abbiamo compreso le grosse
limitazioni che un approccio di questo genere ha.
Per iniziare possiamo notare delle prestazioni da parte del modello non
troppo soddisfacenti.
Una metrica molto utile per comprendere lo scontento è la precisione,
che indica il numero di predizioni corrette sul totale delle predizioni per ogni
classe.
Se quindi per la classe ritenuta sana abbiamo una precisione molto alta, lo stesso
non si puo' dire per quella patologica, che si ferma a circa il 50\%, un valore
troppo basso per un impiego reale.\\
Oltre i problemi di prestazioni, interloquendo con il personale medico
abbiamo idenfiticato ulteriori problemi di natura tecnica.
Il primo, visibile anche il foto, è che l'utilizzo dell'immagine in modalità
bianco e nero faccia perdere moltissime informazioni che potrebbero essere
fondamentali in fase di classificazione.
Ad esempio, come possibile vedere nella figura \ref{fig:gui-one}, un corpo
sicuramente esterno allo stomaco di colore grigio viene in parte
scambiato come tessuto patologico, cosa che difficilmente sarebbe accaduta
utilizzando una versione a colori dell'immagine.\\
Abbiamo poi che l'analisi delle immagini in un secondo momento non risulta
di grande utilità per il personale medico, che ha bisogno di avere un 
riscontro immediato durante l'endoscopia.
Va fatto notare che l'adattamento del modello a funzionare in tempo reale
su un video in input è una strada difficilmente percorribile, in quanto
una prima implementazione dell'algoritmo impiega circa 10 secondi per classificare
la singola immagine.
Quasi l'interità del tempo di elaborazione è speso nel calcolo delle
GLCM e delle GLRLM, dalla mole di calcolo discretamente elevata che aumenta
considerando il grande numero che se ne devono calcolare. \\

In ultimo, grazie al contributo del personale medico, è stato reso evidente
come il considerare le immagini estratte dai 3 filtri che l'I-SCAN utilizza
puo' essere potenzialmente controproducente.
Nello specifico, se nel processo di {\it shuffling}, per pura casualità,
si avesse che vi siano più immagini sane per un dato filtro che per un altro,
potrebbe erroneamente imparare a riconoscere la distribuzione delle
feature estratte da quelle ROI come meno probabilmente patologiche.

\subsection{\label{sec:deep-learning}Tecniche Deep Learning: Reti Neurali Convoluzionali}

Passiamo ora alla seconda possibilità che è stata valutata nello
sviluppo dell'applicativo, che riprende studi nel settore \cite{ilpaper}
che hanno dimostrato ottenere buoni risultati e fanno uso di reti neurali,
ovvero modelli di apprendimento profondo.
Questo significa che a differenza del precedente algoritmo non è possibile
comprendere il processo di classificazione, se non tramite interventi 
specifici all'architettura interna della rete, ma solitamente permettono
anche di ottenere risultati migliori, soprattutto nel caso di analisi di 
immagini in cui hanno sin da principio dimostrato una grossa dominanza 
sui modelli di apprendimento superficiale.

Un generico modello di {\it semantic segmentation} prevede varie
componenti e varie fasi.
Anzitutto il modello utilizza le immagini a colore e non convertite
in bianco e nero, e questo potrebbe dare grossi vantaggi rispetto
alla controparte di apprendimento superficiale.
La fase di {\it pre-processing} è sostanzialmente diversa non solo
nella realizzazione ma anche negli intenti:
se di fatto nell'algoritmo visto in \ref{sec:shallow-learning} lo
scopo era rendere i dati in input quanto più diferibili possibili,
qui invece si punta ad aggiungere disturbi ed alterazioni casuali per
due finalità:
la prima è quella di rendere il modello più generale e robusto,
e quindi più performante;
la seconda è per fini di {\it data-agumentation}, di cui 
parleremo più approfonditamente in \ref{sec:Dataset}.
A questo punto inizia la {\it feature extraction}, che consiste
nell'impiego di varie sequenze di filtri convoluzioni e di operatori
di {\it pooling}.
Il funzionamento generale di questi modelli è piramidale, dove si
esegue filtro $\to$ pooling $\to$ filtro $\to$ pooling $\to$ ...
come nella figura \ref{fig:cnn}, ma l'algoritmo specifico usa
un approccio leggermente diverso e verrà discusso nel dettaglio
in \ref{sec:bisenet}.
Lo scopo dei filtri è estrarre informazioni dall'immagine,
mentre quello degli strati di pooling e comprimere queste
informazioni che sarebbero altrimenti troppo numerose e dunque
intrattabili.

Infine dal risultato degli algoritmi di {\it region proposal}
identificano le zone di rilievo e su queste eseguono la classificazione.
È bene notare che anche questa fase è soggetta ad apprendimento,
ovvero il modello impara nel tempo a riconoscere ROI che potrebbero
essere interessanti o meno, e le zone dell'immagine che non
vengono estratte come ROI sono automaticamente classificate come
classe "sfondo", nel nostro caso "sano".
Inoltre a differenza dell'algoritmo visto in \ref{sec:shallow-learning}
il numero e la dimensione delle ROI estratte non è fisso.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{./assets/cnn.jpg}
    \caption{\label{fig:cnn}Visualizzazione del processo di classificazione di un'immagine. Fonte: https://towardsdatascience.com/region-of-interest-pooling-f7c637f409af}
\end{figure}

Le ROI estratte sono quindi sottoposte a una testa, detto livello
completamente connesso, che etichetta ogni singolo pixel al suo
interno.
È quindi evidente che in questo caso il modello ha il compito,
oltre quello di riconoscere se la ROI sia potenzialemente patologica
o meno, anche di riconoscere quale parte della ROI lo è, e dunque
a livello globale debba apprendere non solo a riconoscere le zone
anomale ma anche a tracciarne la maschera.

\subsection{\label{sec:bisenet}Algoritmo II}

Capito il funzionamento di un generico modello di
{\it semantic segmentation}, seppure ogni architettura sia poi
possa avere nel merito differenze più o meno significative,
possiamo andare a vedere nel dettaglio quella scelta per 
questo specifico caso, ovvero la rete BiseNet \cite{bisenet}.

{\it BiSeNet}\cite{bisenet}, acronimo di
{\it Bilateral Segmentation Network}, prende il nome per via
della sua architettura interna che si dirama in due parti.
Come discusso in precedenza la caratteristica fondamentale
delle reti neurali che le rende così accurate nel riconoscere
le immagini è la fase di estrazione delle {\it features} che avviene
tramite filtri convoluzionali e pooling.
Un filtro convoluzionale puo' avere varie caratteristiche,
tra cui la sua dimensione, la sua profondità, il suo {\it stride},
ovvero quanto si sposta in orizzontale o in verticale ad ogni
iterazione, e la sua {\it padding}, ovvero se si aggiungono
zeri intorno all'immagine per farla avere dimensioni multipli
della dimensione del filtro, ma soprattuo il numero di filtri da
applicare.
In linea generale si ha che con un numero più elevato di filtri
le prestazioni siano migliori, ma anche che il processo
sia più pesante computazionalmente parlando e dunque più
lento, cosa che va tenuta in considerazione nel momento in cui 
si tenti, come nel nostro caso, di realizzare un sistema che funzioni
in tempo reale.


\begin{figure}
    \center
    \includegraphics[width=0.9\textwidth]{./assets/bisenet.jpg}
    \caption{\label{fig:bisenet}Visualizzazione ad alto livello dell'architettura di BiSeNet. Fonte: researchgate.net}
\end{figure}

In letteratura si ha che per architetture più veloci, che tentano
di raccogliere più informazioni di alto livello a scapito di quelle
specifiche e locali, si utilizzino tali filtri in maniera ridotta,
preferendo la codifica delle informazioni tramite l'uso più esteso
di livelli di pooling, mentre in contesti in cui si cerca di
cogliere in maniera più fine i dettagli si utilizzino di più i filtri.
BiSeNet tenta di fondere i due approcci, utilizzando un primo ramo
che è chiamato {\it context path} (CP) e che si occupa di estrarre
informazioni di alto livello, e un secondo ramo che è chiamato
{\it spatial path} (SP) e che si occupa invece di estrarre informazioni
di basso livello, utilizzando le tecniche detto per ora.
Infine i due rami vengono uniti tramite dei
{\it feature fusion module} (FFM) e degli
{\it attention refinement module}.
Il secondo si occupa di unire più stadi della CP al fine di ottenerne
uno finale più accurato, tramite dei meccanismi di attenzione che sono
una tecnica auto-supervisione dei modelli per apprendere quali 
informazioni tra quelle estratte siano più rilevanti di altre,
mentre il primo esegue varie operazioni matematiche tra i due
risultati, tipo la somma o il prodotto componente per componente, 
in modo da ottenre un unico risultato finale.
Questo risultato viene infine valutato e ridimensionato in modo
da ottenere una dimensione pari a quella in ingresso.


%% \begin{itemize}
    %% \item Ridimensionamento a 800$\times$800
    %% \item Ritaglio casuale di una porzione 600$\times$600
    %% \item Rotazione 180° con probabilità $p=0.5$
    %% \item Distorsione fotometrica casuale con probabilità $p=0.5$
    %% \item Normalizzazione lungo le componenti RGB.
%% \end{itemize}

%% L'unica trasformazione finalizzata a migliorare il processo di
%% {\it training} è la normalizzazione lungo le componenti RGB,
%% che normalizza i canali in modo da avere media pari a
%% $[123.7, 116.3, 103,5]$ e deviazione standard pari a
%% $[58.4, 57.1, 57.4]$.
%% Questo rende l'immagine in input più simile a quella che è
%% la percezione dell'occhio umano, essendo le intensità percepite
%% dei tre colori differenti tra loro.


\section{\label{sec:Dataset}Dataset}

Allenando un modello di intelligenza artificiale è necessario
rifelttere su quali dati si stiano utilizzando.
Il già grande e tutt'ora crescente interesse per la disciplina
nel mondo della ricerca ha infatti portato alla raccolta
e pubblicazione di grandi dataset per sperimentare
nuove tecniche e avere dei confronti di prestazioni
normalizzati tra i differenti modelli.
Tuttavia, puntanto a costruitre un applicativo che possa essere
considerato come qualcosa di concretamente reale, non è
spesso possibile utilizzare questi dataset, perché
spesso non sono disponibili dati specifici per l'applicativo
in considerazione o quelli disponibili non sono rappresentativi
del nostro caso.

Ci sono anche casi in cui si dispone invece dei dati, ma
essi sono semplicemente troppo pochi.
sebbene tecniche di {\it data augmentation}, ossia tecniche
che puntano ad aumentare in maniera artificiale la quantità
di dati di cui si discone, portando in realtà anche un
aumento della qualità del modello prodotto, siano la norma
nello sviluppo di un modello, se comunque i dati a disposizione
sono in partenza di quantità molto scarsa non è possibile
sperare di ottenere un modello robusto e affidabile.
Per questo motivo in questo progetto sono stati usati due
dataset, uno pubblico e noto all'interno della comunità
della ricerca e l'altro composto da immagini raccolte e
annotate dall'Ospedale Mauriziano di Torino.

\subsection{\label{sec:dataset-mauriziano}Il Dataset Mauriziano}

Il dataset si compone di 113 immagini di esami endoscopici
raccolte dall'ospedale Medico durante le loro attività,
di cui 51 con aree patologiche e 62 senza.
Si presentano come acquisizioni del macchinario I-Scan,
e quindi sono immagini ad altissima risoluzione (1920$\times$1080 pixel).
Essendo però acquisizioni del macchinario, presentano anche
una grandissima quantità di informazioni superflue o non utilizzabili,
come grosse porzioni di immagini nere che in realtà fanno parte
dell'interfaccia oppure una seconda visuale ausiliaria che non
puo' essere impiegata al fine dell'addestramento del modello.
Questo porta la porzione di immagine realmente utilizzabile
a circa 1250$\times$1020, una risoluzione comunque molto
elevata.

Tuttavia, per una questione numerica, questo non puo' essere
l'ultimo dataset impiegato.
Delle 51 immagini patologiche annotate infatti di solo 
18 si dispone anche dell'immagine originale non annotata.
Va fatto notare che le annotazioni sono state eseguite in maniera
{\it hard coded} sull'immagine, e non è quindi separabile dall'immagine
originale.
Questo non è un problema per l'addestramento del modello descritto
nella sezione \ref{sec:shallow-learning} in quanto la conversione
dell'immagine in scala di grigi e l'utilizzo di porzioni di immagini
molto ridotte non permette al modello di discriminare le ROI
in base alla presenza o meno di questa annotazione.
Nel caso del modello descritto nella sezione \ref{sec:deep-learning}
invece è molto possibile che il modello impari a proporre le ROI,
se non direttamente a classificarle in base alla presenza
dell'annotazione, in quanto sarebbe in grado di riconoscere
la differenza di colore e discriminare le ROI in base a questo.

\begin{figure}[h]
    \center
    \includegraphics[width=0.9\textwidth,trim=5.0cm 5cm 6cm 6.0cm,clip]{./assets/cutout.png}
    \caption{\label{fig:cutout}Processo di estrazione di una ROI. Da sinistra a destra:
    Immagine originale, maschera calcolata, visualizzazione della sola area patologica
    }
\end{figure}

La creazione della maschera è possibile tramite delle semplici
considerazioni sulla natura delle annotazioni, in quanto sono
state fatte con l'utilizzo di un verde tipicamente ad alto
contrasto con il resto delle immagini.
Per via di questa caratteristica è facile ottenere una prima
{\it bitmap} del marchio apposto sulla foto.
Successivamente si applicano in sequenza i seguenti operatori
binari:

\begin{itemize}
    \item {\it Dilatazione}: chiusura di eventuali aree non congiunte
    \item {\it Chiusura}: riempire il centro dell'annotazione
    \item {\it Erosione} $\times$ 2: annullamento della precedente dilatazione
    e restringimento della maschera all'interno dell'annotazione
\end{itemize}

Nel caso dell'algoritmo in \ref{sec:shallow-learning} dalle immagini
elaborate vengono estratte ROI di dimensione 50 $\times$ 50 pixel,
con una sovrapposizione del 40\%, ovvero 20 pixel.
Per aumentare la quantità di ROI patologiche di cui si è a disposizione
esse vengono inoltre salvate anche rotate di 90°.
Questo permette di estrarre in totale circa 210'000 campioni su cui allenare
il modello.
È bene notare che nonostante questo tentativo di aumentare le ROI
patologiche viene applicato alle immaigni non-patologiche un processo
di {\it oversampling}, ovvero in fase di estrazione delle {\it features}
alcune ROI sane vengono estratte più di una volta, in quanto questo
ha sperimentalmente prodotto un modello più oggettivo nelle sue
valutazioni.

\begin{table}
    \center
    \begin{tabular}[h]{||c||c|c|c||}
        \hline
        & Train & Test & Totale \\
        \hline
        Immagini Sane & 40 & 7 & 47 \\
        \hline
        Imagini Patologiche & 42 & 6 & 48 \\
        \hline
        ROI Sane & 84'847 & 6'657 & 91'504 \\
        \hline
        ROI Patologiche & 54'278 & 1'620 & 55'898\\
        \hline
        \hline
        \multirow{2}{*}{Totale} & 82 & 13 & 95 \\ \cline{2-4}
        & 149'548 & 64'442 & 213'990 \\
        \hline
    \end{tabular}
    \caption{\label{tab:mauriziano}Composizione del dataset Mauriziano}
\end{table}

Nel caso dell'algoritmo in \ref{sec:deep-learning} invece, le immagini
vengono solo ridimensionate a una dimensione di di 570 $\times$ 570 pixel,
ruotate orizzontalme in maniera casuale con una probabilità del 50\%.
Questo è fatto perché vengono utilizzate assieme alle immagini
dell'altro dataset e devono quindi avere la stessa dimensione.
Data la risoluzione ridotta delle immagini, è più conveniente
ridimensionare queste che invece sono di risoluzione più alta.

\begin{figure}
    \center
    \includegraphics[width=0.9\textwidth]{./assets/roi-shallow.png}
    \caption{\label{fig:roi-shallow} Esempio di ROI estratta per algoritmo di shallow learning (sinistra) e
    di versione digitalizzata su 5 livelli di grigio per il calcolo di GLCM e GLRLM (destra).}
\end{figure}

\subsection{Il Dataset HyperKvasir}

Il dataset HyperKvaris\cite{HyperKvasirDataset} nasce per sopperire
alla mancanza di dati medici per lo studio e l'addestramento
di modelli di intelligenza artificiale.
La raccolta di dati è, in ogni ambito, un lavoro costoso non 
soltatno in termini monetari ma spesso e volentieri anche
per fattori di tempo e di risorse umane.
Dataset comunemente usati nella ricerca di modelli di intelligenza
artificiale, come {\it Common Object and Context}\cite{cocodataset}
(COCO) ed {\it ImageNet} \cite{imagenet} contengono
immagini di oggetti comuni presi dalla quotidianità,
quindi classificabili da qualunque persona: ciò non toglie
che il processo di annotazione, che puo' prendere
varie forme come la classificazione, la segmentazione e la
circoscrizione, sia un processo lungo e ripetitivo.

Nel caso specifico della medicina, le cose si complicano
molto di più.
Sin dal principio, è molto diffile ottenere i dati grezzi,
quindi non annotati, spesso anche per tutelare la privacy
del paziente stesso, o comunque è difficile ottenerne in
quantità sufficienti, ed anche quando questo accade
è necessario l'impiego di personale medico altamente
specializzato, nel nostro caso nel settore della
gastroenterologia, per annotare e segmentare le immagini.
Specificiatamente per l'ambito medico, è anche preferibile
avere più di un riscontro.

HyperKvasir è dunque la più grande raccolta di immagini
e video di esami di endoscopia digestiva, rilasciato proprio
allo scopo di facilitare la ricerca in questo ambito.
Va fatto tuttavia notare che quanto detto in precedenza resta
purtroppo vero:
non lasciandoci trarre in inganno dalla dimensione del dataset,
che ammonta a ben 110'00' immagini e 374 video, bisogna
constatare che si tratti comunque di un dataset molto
eterogeno nella sua composizione.
Delle 110'000 immagini, infatti, 99'000 sono che sono
talmente prive di annotazioni, vale a dire nè classificate
nè segmentate in alcun modo, e delle restanti 11'000
solo 1'000 sono immagini di cui sono disponibili
le annotazioni di segmentazione.
Per ultimo, trattandosi di un dataset per lo studio
di analisi endoscopiche, le immagini riportanti metaplasie
sono presenti solo come immagini classificate ma non come
segmentate, fattore che renderebbe impraticabile
l'utilizzo del nostro modello.

\begin{table}
    \center
    \begin{tabular}{||c|c|c|c||}
        \hline
        Tipologia & \# Campioni & \# Classi & Dimensione (MB) \\
        \hline
        \hline
        Classificate & 10'662 & 23 & 3'960 \\
        \hline
        Segmentate & 1'000 & 1 (Polyp) & 57 \\
        \hline
        Non annotate & 99'417 & 0 & 29'940 \\
        \hline
        Video & 374 & 30 & 32'539 \\
        \hline
        \hline
        Totale & 111'453 & --- & 66'496 \\
        \hline
    \end{tabular}
    \caption{\label{tab:hyperkvasir}Composizione del dataset HyperKvasir}
\end{table}

È ovviamente improprio l'utilizzo di un dataset che raffiguri
oggetti diversi da quelli che si possono riscontrare nel 
contesto di applicazione per l'addestramento del modello,
tuttavia avendo a che fare con un numero di campioni
relativo ad esso molto limitato si è scelto di utilizzarlo
in congiunzione con il dataset descritto in \ref{sec:dataset-mauriziano}.
Questo è possibile tramite l'impiego del {\it transfer learning},
che consiste nell'usare come base di partenza per l'addestramento
di un nuovo modello uno già allenato su un dataset con
caratteristiche simili al nuovo.
Questa tecnica è generalmente utilizzata per velocizzare
il processo di apprendimento, insieme anche al raggiungimento
di risultati migliori, ma è anche molto utile quando si 
dispone di pochi dati, come nel nostro caso.
È bene notare dunque che l'impiego di {\it transfer learning}
per lo sviluppo di modelli di intelligenza artificiale non è
dunque un ultima risorsa ma bensì la norma, vista la
pervasività che ha dimostrato nei vari ambiti di applicazione.

In ultimo, il dataset HyperKvasir è anche molto eterogeno
nella forma in cui si presentano le immagini.
A differenza del dataset in \ref{sec:dataset-mauriziano} infatti,
dove le acquisizioni sono state effettuate tutte con la medesima
macchina, sono di uguale ed altissima risoluzione, nel caso
di HyperKvasir infatti abbiamo con immagini provenienti da diverse
fonti e di qualità generalmente inferiore.

%% \begin{table}
    %% \center
    %% \begin{tabular}{||c|c|c|c|c||}
        %% \hline
        %% & W < 400 & 400 < W < 600 & 600 < W < 800 & W > 800 \\ 
        %% \hline
        %% H < 400 &  &  &  & \\ 
        %% \hline
        %% 400 < H < 800 &  &  &  & \\ 
        %% \hline
        %% 600 < H < 800  &  &  &  & \\ 
        %% \hline
        %% H > 800 & &  &  & \\ 
        %% \hline
    %% \end{tabular}
%% \end{table}

\section{Processi}
\subsection{La Libreria MMSegmentation}
\subsection{Allenamento del Modello}
\subsection{Valutazione del Modello}